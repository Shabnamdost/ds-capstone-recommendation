{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with Surprise\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept\n",
    "Surprise is a Python SciKit that comes with various recommender algorithms and similarity metrics to make it easy to build and analyze recommenders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"nmf.png\" width=\"600\" height=\"400\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries\n",
    "***Implementing the Surprise Library to Model a Recommender System***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# required libs and packages \n",
    "\n",
    "import sys\n",
    "import time # for counting the time for each steps\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "from surprise import KNNWithMeans, Dataset, Reader\n",
    "from surprise.model_selection import cross_validate, GridSearchCV\n",
    "import time\n",
    "# loading similarity search model\n",
    "\n",
    "from surprise.prediction_algorithms import KNNWithMeans, SVD, SVDpp, SlopeOne, NMF, NormalPredictor, KNNBaseline, KNNBasic, KNNWithMeans, KNNWithZScore, BaselineOnly, CoClustering\n",
    "\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of the WorkFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def perform_grid_search_ratings(file_path, model_gscv, sim_options, measures=[\"rmse\", \"mae\"], cv):\n",
    "    \"\"\"\n",
    "    Function to perform a grid search on KNNWithMeans algorithm for collaborative filtering.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the CSV file containing the dataset.\n",
    "    - sim_options: dict, dictionary of similarity options for KNN.\n",
    "    - param_grid: dict, grid of hyperparameters for grid search.\n",
    "    - measures: list, list of evaluation measures.\n",
    "    - cv: int, number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "    - gs: Fitted GridSearchCV object with results.\n",
    "    - time_elapsed: Time taken for the grid search.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load and preprocess the dataset\n",
    "    df_rating = pd.read_csv(file_path)\n",
    "    df_rating.rename(columns={'userId': 'user', 'movieId': 'item'}, inplace=True)\n",
    "    \n",
    "    # Convert the DataFrame to a Surprise dataset format\n",
    "    reader = Reader(rating_scale=(df_rating['rating'].min(), df_rating['rating'].max()))\n",
    "    data = Dataset.load_from_df(df_rating[[\"user\", \"item\", \"rating\"]], reader)\n",
    "    \n",
    "    # Set default similarity options if not provided\n",
    "    if sim_options is None:\n",
    "        sim_options = {\n",
    "            \"name\": [\"msd\", \"cosine\"],\n",
    "            \"min_support\": [3, 4, 5],\n",
    "            \"user_based\": [False, True],\n",
    "        }\n",
    "    \n",
    "    # Set default parameter grid if not provided\n",
    "    # if param_grid is None:\n",
    "    param_grid = {\"sim_options\": sim_options}\n",
    "    \n",
    "    # Perform grid search\n",
    "    time_start = time.time()\n",
    "    gs = GridSearchCV(model_gscv, param_grid, measures=measures, cv=cv)\n",
    "    gs.fit(data)\n",
    "    time_elapsed = time.time() - time_start\n",
    "    \n",
    "    return gs, time_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fun_Recommender(test_user, df_rating, df_movies, model, no_recom):\n",
    "    \"\"\"Making a function which gets the user, df_rating, and movies and optimized model\n",
    "    and return a recommendation list \"\"\"\n",
    "    # here we predict the ratings by one user on all the movies. The user is 10\n",
    "    user_predic = [] # a list which stores the predicted rates\n",
    "    for  movie in df_rating['item'].unique():\n",
    "        prediction = model.predict(test_user, movie)\n",
    "        user_predic.append(prediction.est)\n",
    "    \n",
    "    # Get the data for the user and rating and turn them into a panda dataframe\n",
    "    dict_user = {'movieId': df_rating['item'].unique(), 'rating': user_predic}\n",
    "    df_user = pd.DataFrame(dict_user)\n",
    "\n",
    "    \n",
    "    dict_user = {'movieId': df_rating['item'].unique(), 'rating': user_predic}\n",
    "    df_user = pd.DataFrame(dict_user)\n",
    "\n",
    "    # first searching for movies rated by the user\n",
    "    list_seen_movies = df_rating[df_rating['user'] == user]['item'].tolist()\n",
    "    \n",
    "    df_user_recom = df_user.copy()\n",
    "    # here we set the rating for seen movies by the user to zero\n",
    "    for i in range(df_user_recom.shape[0]):\n",
    "        if df_user_recom.loc[i, 'movieId'] in list_seen_movies:\n",
    "            df_user_recom.loc[i, 'rating'] = 0\n",
    "    df_user_recom_sorted = df_user_recom.sort_values(by='rating', ascending = False)\n",
    "    recommendations_movie=df_user_recom_sorted['movieId']\n",
    "    return df_movies.set_index('movieId').loc[recommendations_movie].head(no_recom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "def get_movie_posters(movie_tmbds, api_key, save_directory='./images'):\n",
    "    \"\"\"\n",
    "    Function to get movie posters' URLs and save the images locally.\n",
    "    \n",
    "    Parameters:\n",
    "    - movie_tmbds: list of int, a list of TMDB movie IDs.\n",
    "    - api_key: str, TMDB API key.\n",
    "    - save_directory: str, path to the directory where images will be saved (default is './images').\n",
    "\n",
    "    Returns:\n",
    "    - list_poster_url: list of str, list of poster URLs.\n",
    "    \"\"\"\n",
    "    # Ensure the directory for saving images exists\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    image_paths = []\n",
    "    \n",
    "    for movie_tmbd in movie_tmbds:\n",
    "        print(f\"Fetching poster for movie ID: {movie_tmbd}\")\n",
    "        base_url = f'https://api.themoviedb.org/3/movie/{movie_tmbd}'\n",
    "\n",
    "        # Send a GET request to TMDB API\n",
    "        response = requests.get(base_url, params={'api_key': api_key})\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            poster_path = data.get('poster_path')\n",
    "            if poster_path:\n",
    "                poster_url = f'https://image.tmdb.org/t/p/w500{poster_path}'\n",
    "                # list_poster_url.append(poster_url)\n",
    "                \n",
    "                # Send a GET request to the image URL\n",
    "                response_image = requests.get(poster_url)\n",
    "                image = Image.open(BytesIO(response_image.content))\n",
    "                \n",
    "                # Save the image locally\n",
    "                image_path = os.path.join(save_directory, f\"poster_tmbd_{movie_tmbd}.jpg\")\n",
    "                image.save(image_path)\n",
    "                image_paths.append(image_path)\n",
    "                print(f\"Saved poster for movie ID {movie_tmbd} at {image_path}\")\n",
    "            else:\n",
    "                print(f\"Poster not found for movie ID {movie_tmbd}.\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch movie details for ID {movie_tmbd}. Status code: {response.status_code}\")\n",
    "    \n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_image_merger(image_paths, show_type):\n",
    "    \"\"\" It gets in paths of images and merge the images vertically (ver) or horizontally (hor)\"\"\"\n",
    "    # image_paths is a list of paths of images\n",
    "    # show_type is direction to show the merge image\n",
    "\n",
    "    # Open images and store them in a list\n",
    "    images = [Image.open(img_path) for img_path in image_paths]\n",
    "\n",
    "    # Determine the width and height for the final merged image\n",
    "    # For horizontal merge\n",
    "    # calculating the total width width and maximum hight of all images\n",
    "    total_width = sum(img.width for img in images)\n",
    "    max_height = max(img.height for img in images)\n",
    "\n",
    "    # For vertical merge\n",
    "    # calculating the total hights width and maximum wdith of all images\n",
    "    total_height = sum(img.height for img in images)\n",
    "    max_width = max(img.width for img in images)\n",
    "\n",
    "    # Create a new blank image for horizontal merge\n",
    "    merged_image_horizontal = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "    # Create a new blank image for vertical merge\n",
    "    merged_image_vertical = Image.new('RGB', (max_width, total_height))\n",
    "\n",
    "    # Paste images side by side for horizontal merge\n",
    "    x_offset = 0\n",
    "    for img in images:\n",
    "        merged_image_horizontal.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "\n",
    "    # Paste images on top of each other for vertical merge\n",
    "    y_offset = 0\n",
    "    for img in images:\n",
    "        merged_image_vertical.paste(img, (0, y_offset))\n",
    "        y_offset += img.height\n",
    "\n",
    "# Save the merged images\n",
    "    merged_image_horizontal.save('./images/merged_image_horizontal.jpg')\n",
    "    merged_image_vertical.save('./images/merged_image_vertical.jpg')\n",
    "    \n",
    "    if show_type == 'hor':\n",
    "        return merged_image_horizontal.show()\n",
    "    else:\n",
    "        return merged_image_vertical.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Tables: Getting the Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load_data.py\n",
    "\n",
    "# first we read our data set from directroy\n",
    "\n",
    "df_rating = pd.read_csv('./data/ml-latest-small/ratings.csv')\n",
    "df_movies = pd.read_csv('./data/ml-latest-small/movies_modified.csv')\n",
    "\n",
    "df_links = pd.read_csv('./data/ml-latest-small/links.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fun_min_rating_out(df, min_user_ratings, min_movie_ratings):\n",
    "    \"\"\" It reduce the size of data set by dropping the userId with rating lower than a thershhold\n",
    "    and movies rated lower than a thershdold\"\"\"\n",
    "    #  min_user_ratings: minimum number rating per user\n",
    "    # min_movie_ratings: minium number of rating per movies\n",
    "    # min_user_ratings = 20\n",
    "    filter_users = df['userId'].value_counts() > min_user_ratings\n",
    "    filter_users = filter_users[filter_users].index.tolist()\n",
    "\n",
    "\n",
    "    # min_movie_ratings = 20\n",
    "    filter_movies = df['movieId'].value_counts() > min_movie_ratings\n",
    "    filter_movies = filter_movies[filter_movies].index.tolist()\n",
    "    df_new = df[(df['movieId'].isin(filter_movies)) & (df['userId'].isin(filter_users))]\n",
    "    \n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data frame shape:\t(100836, 4)\n",
      "The new data frame shape:\t(66405, 4)\n"
     ]
    }
   ],
   "source": [
    "# checking to see if the function fun_min_rating_out can drop unnecessary observavtions\n",
    "print('The original data frame shape:\\t{}'.format(df_rating.shape))\n",
    "\n",
    "print('The new data frame shape:\\t{}'.format(fun_min_rating_out(df_rating, 20, 20).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Tables: Getting the right data set for the Surprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x1e26de37f10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rating_red = fun_min_rating_out(df_rating, 20, 20)\n",
    "print(df_rating_red.head())\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "data = Dataset.load_from_df(df_rating_red[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benschmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test_rmse  fit_time  test_time Algorithm\n",
      "0    0.84893  0.557259   0.151026       SVD\n",
      "   test_rmse   fit_time  test_time Algorithm\n",
      "0   0.835842  13.629437   4.143286     SVDpp\n",
      "   test_rmse  fit_time  test_time Algorithm\n",
      "0    0.85203  0.411435   2.447098  SlopeOne\n",
      "   test_rmse  fit_time  test_time Algorithm\n",
      "0   0.876786  0.932221   0.145815       NMF\n",
      "   test_rmse  fit_time  test_time        Algorithm\n",
      "0   1.367854  0.062498   0.093739  NormalPredictor\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "   test_rmse  fit_time  test_time    Algorithm\n",
      "0   0.841798  0.229144   2.057142  KNNBaseline\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "   test_rmse  fit_time  test_time Algorithm\n",
      "0   0.904195  0.135416   1.541544  KNNBasic\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "   test_rmse  fit_time  test_time     Algorithm\n",
      "0   0.851837  0.156232   1.676322  KNNWithMeans\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "   test_rmse  fit_time  test_time      Algorithm\n",
      "0   0.851475  0.192694    1.85185  KNNWithZScore\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "   test_rmse  fit_time  test_time     Algorithm\n",
      "0   0.849464  0.109367   0.119779  BaselineOnly\n",
      "   test_rmse  fit_time  test_time     Algorithm\n",
      "0   0.895987  1.328033    0.15103  CoClustering\n"
     ]
    }
   ],
   "source": [
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "for algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n",
    "    # Get results & append algorithm name\n",
    "    \n",
    "    tmp = pd.DataFrame(results) #.mean(axis=1)\n",
    "    tmp = pd.DataFrame(results) #.mean(axis = 0 )\n",
    "    tmp.iloc[0,0] = tmp.iloc[:,0].mean()\n",
    "    tmp.iloc[0,1] = tmp.iloc[:,1].mean()\n",
    "    tmp.iloc[0,2] = tmp.iloc[:,2].mean()\n",
    "\n",
    "    tmp = tmp.drop([1,2], axis = 0)\n",
    "\n",
    "\n",
    "    new_data = pd.Series([str(algorithm).split(' ')[0].split('.')[-1]] , name='Algorithm')\n",
    "\n",
    "    tmp = pd.concat([tmp, new_data], axis=1)\n",
    "    print(tmp)\n",
    "\n",
    "    benchmark.append(tmp)\n",
    "   \n",
    "# pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVDpp</th>\n",
       "      <td>0.835842</td>\n",
       "      <td>13.629437</td>\n",
       "      <td>4.143286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBaseline</th>\n",
       "      <td>0.841798</td>\n",
       "      <td>0.229144</td>\n",
       "      <td>2.057142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>0.848930</td>\n",
       "      <td>0.557259</td>\n",
       "      <td>0.151026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaselineOnly</th>\n",
       "      <td>0.849464</td>\n",
       "      <td>0.109367</td>\n",
       "      <td>0.119779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithZScore</th>\n",
       "      <td>0.851475</td>\n",
       "      <td>0.192694</td>\n",
       "      <td>1.851850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithMeans</th>\n",
       "      <td>0.851837</td>\n",
       "      <td>0.156232</td>\n",
       "      <td>1.676322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SlopeOne</th>\n",
       "      <td>0.852030</td>\n",
       "      <td>0.411435</td>\n",
       "      <td>2.447098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF</th>\n",
       "      <td>0.876786</td>\n",
       "      <td>0.932221</td>\n",
       "      <td>0.145815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoClustering</th>\n",
       "      <td>0.895987</td>\n",
       "      <td>1.328033</td>\n",
       "      <td>0.151030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBasic</th>\n",
       "      <td>0.904195</td>\n",
       "      <td>0.135416</td>\n",
       "      <td>1.541544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NormalPredictor</th>\n",
       "      <td>1.367854</td>\n",
       "      <td>0.062498</td>\n",
       "      <td>0.093739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 test_rmse   fit_time  test_time\n",
       "Algorithm                                       \n",
       "SVDpp             0.835842  13.629437   4.143286\n",
       "KNNBaseline       0.841798   0.229144   2.057142\n",
       "SVD               0.848930   0.557259   0.151026\n",
       "BaselineOnly      0.849464   0.109367   0.119779\n",
       "KNNWithZScore     0.851475   0.192694   1.851850\n",
       "KNNWithMeans      0.851837   0.156232   1.676322\n",
       "SlopeOne          0.852030   0.411435   2.447098\n",
       "NMF               0.876786   0.932221   0.145815\n",
       "CoClustering      0.895987   1.328033   0.151030\n",
       "KNNBasic          0.904195   0.135416   1.541544\n",
       "NormalPredictor   1.367854   0.062498   0.093739"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([benchmark[i].iloc[0,:] for i in range(len(benchmark)) ], columns = ['test_rmse', 'fit_time', 'test_time', 'Algorithm']).set_index('Algorithm').sort_values('test_rmse', ascending = True)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
